name: Protein Structure Prediction using Deep Convolutional LSTM's

#initiate workflow on push or pull from master branch
on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]
    types: [assigned, opened, synchronize, reopened]

  #allow for workflow to be manually initiated from the Actions tab
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.6, 3.7, 3.8, 3.9]   #testing on multiple python versions
    steps:
     - uses: actions/checkout@v2
     - name: Set up Python ${{ matrix.python-version }}
       uses: actions/setup-python@v2
       with:
         python-version: ${{ matrix.python-version }}
         token: ${{ secrets.CODECOV_TOKEN }} # not required for public repos
         files: ./coverage1.xml,./coverage2.xml 
         flags: unittests 
         name: codecov-umbrella 
         fail_ci_if_error: true 
         verbose: true 

    #install all required dependancies for project, including workflow specific requirements
     - name: Install Python dependencies
       run: |
        echo "Installing Dependancies"
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        python -m pip install --upgrade pip
        pip3 install flake8 pytest
        pip3 install codecov
        pip3 install coverage
        pip3 install pytest
        pip3 install pytest-cov
        pip3 install bandit
        pip3 install safety
        pip3 install google-cloud
        pip3 install google-cloud-storage
        pip3 install Cython
        sudo apt install graphviz
    #run package safety check to look for known security vulnerabilities in packages
     - name: Package safety check
       run: |
        echo "Running package safety check"
        python3 -m safety check > package_safety_output.txt
        cat package_safety_output.txt
       continue-on-error: true

    #run Bandit security check for any known vulnerabilities in code
     - name: Bandit
       run: |
        echo "Running Bandit"
        python3 -m bandit -r . > bandit_output.txt
        cat bandit_output.txt
       continue-on-error: true

    #decrypt GCP Service Account gpg key for testing GCP pipeline
     # - name: Decrypting Service Account
     #   env:
     #    PASSPHRASE: ${{ secrets.PASSPHRASE }}
     #   run: |
     #    chmod +x ./.github/scripts/decrypt_secret.sh
     #    ./.github/scripts/decrypt_secret.sh
     #    cat $HOME/secrets/my_secret.json
     #    echo "GOOGLE_APPLICATION_CREDENTIALS=$HOME/secrets/my_secret.json" >> $GITHUB_ENV

    #install all required GCP dependancies for testing GCP pipeline
     - name: Install GCP Dependancies
       run: |
        echo "Installing all GCP Dependancies"
        echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
        sudo apt-get install apt-transport-https ca-certificates gnupg
        curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
        sudo apt-get update && sudo apt-get install google-cloud-sdk
      # gcloud auth activate-service-account --key-file=$HOME/secrets/my_secret.json
      # gcloud config set project ${{ secrets.GCP_PROJECT }}
      # gcloud config configurations list
      # gcloud auth list

    #run flake8 code linter on project
     - name: Lint with flake8
       run: |
        echo "Running flake8"
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics > flake8_output.txt
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics >> flake8_output.txt
        gsutil cp flake8_output.txt "${{ secrets.GCP_BUCKET }}/workflow_logs/flake8_logs/flake8_output_$(date +"%H:%M_%m_%d_%Y").txt"
       continue-on-error: true

    #upload artifacts and logs from workflow to GCP
     # - name: Upload to GCP
     #   run: |
     #    echo "Uploading artifacts to GCP"
     #    gsutil cp bandit_output.txt "${{ secrets.GCP_BUCKET }}/workflow_logs/bandit_logs/bandit_output_$(date +"%H:%M_%m_%d_%Y").txt"
     #    gsutil cp package_safety_output.txt "${{ secrets.GCP_BUCKET }}/safety_logs/bandit_logs/package_safety_output_$(date +"%H:%M_%m_%d_%Y").txt"
     #    gsutil cp flake8_output.txt "${{ secrets.GCP_BUCKET }}/workflow_logs/flake8_logs/flake8_output_$(date +"%H:%M_%m_%d_%Y").txt"

    #run dummy model to test local pipeline
     - name: Run Dummy Model locally
       run: |
        echo "Running dummy psp model"
        python3 main.py --config="dummy"
    #run dummy model to test GCP pipeline
     - name: Run Dummy Model on GCP
       run: |
        echo "Running dummy psp model on GCP"
      # ./gcp_training.sh --config="dummy" --local=1

    #run all unittests
     - name: Testing with unittest
       run: |
        echo "Testing all tests using unittest framework..."
        python3 -m unittest discover -b
    #run coverage report
    #  - name: Coverage Report
    #    run: |
    #     echo "Running coverage report..."
    #     pytest --cov-report term --cov=test_dcblstm tests/
    
     #running code cov 
     - name: Code Cov
     #uses: actions/checkout@master
      #  uses: codecov/codecov-action@v2
       run: |
        curl -Os https://uploader.codecov.io/latest/linux/codecov
        chmod +x codecov
        ./codecov -t ${{ secrets.CODECOV_TOKEN }}
